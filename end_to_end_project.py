# -*- coding: utf-8 -*-
"""End_to_End_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RgwQMLMmsws5fVX431wgYO5_22GU-mxI
"""

import urllib

urllib.request.urlretrieve("https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz", "housing.tgz")

import tarfile

housing_tgz = tarfile.open('housing.tgz')

housing_tgz.extractall()

housing_tgz.close()

import pandas as pd

housing = pd.read_csv('housing.csv')

housing.head()

housing.columns.values

housing.info()

housing.describe()
housing['ocean_proximity'].unique()
housing.isnull().sum()

housing.shape

import numpy as np
import matplotlib.pyplot as plt

housing.hist(bins=50, figsize=(20,15))

corr_matrix = housing.corr(numeric_only=True)

corr_matrix['median_house_value'].sort_values(ascending=False)

housing['income_cat']= np.ceil(housing['median_income']/1.5)

housing['income_cat'].where(housing['income_cat']<5, 5, inplace=True)

housing['income_cat'].hist()

from sklearn.model_selection import StratifiedShuffleSplit

sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_index, test_index in sss.split(housing, housing['income_cat']):
    train_set = housing.iloc[train_index]
    test_set = housing.iloc[test_index]

train_set.shape

#train_set.info()

train_set = train_set.drop("income_cat", axis=1)

test_set = test_set.drop("income_cat", axis=1)

train_set.shape

housing = train_set.copy()


corr_matrix = housing.corr(numeric_only=True)

corr_matrix['median_house_value'].sort_values(ascending=False)

housing = train_set.drop("median_house_value", axis=1)

housing_label = train_set["median_house_value"].copy()  # Target or y column

housing.info()

from sklearn.base import BaseEstimator, TransformerMixin

#hard code column index
rooms_ix, bedrooms_ix, population_ix,household_ix = 3, 4,5,6

class CombineAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self,add_bedrooms_per_rooms=True):
        self.add_bedrooms_per_rooms = add_bedrooms_per_rooms
    def fit(self, X, y=None):
        return self
    def transform(self, X, y=None):
        rooms_per_household = X[:,rooms_ix]/X[:,household_ix]
        population_per_household = X[:,population_ix]/X[:,household_ix]
        
        if self.add_bedrooms_per_rooms:
            bedrooms_per_room = X[:,bedrooms_ix]/X[:,rooms_ix]
            return np.c_[X, rooms_per_household,population_per_household,bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household,population_per_household]

#att_adder = CombineAttributesAdder()

#att =att_adder.transform(housing.values)

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

housing_num = housing.drop("ocean_proximity", axis=1)

#create the num pipeline
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="median")),
    ('attrib_adder',CombineAttributesAdder()),
    ('std_scaler', StandardScaler())
    
])

housing_num_tr = num_pipeline.fit_transform(housing_num)

from sklearn.compose import ColumnTransformer

from sklearn.preprocessing import OneHotEncoder

num_attribs = list(housing_num)

num_attribs

cat_attribs = ['ocean_proximity']

full_pipeline = ColumnTransformer([
    ("num", num_pipeline, num_attribs ),
    ("cat", OneHotEncoder(), cat_attribs)
])

housing_prepared = full_pipeline.fit_transform(housing)

df_new = pd.DataFrame(housing_prepared)
df_new.info()
#11-15 onehot

#Calculate RMSE
from sklearn.metrics import mean_squared_error


#Random Forest Model

from sklearn.ensemble import RandomForestRegressor

forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)

forest_reg.fit(housing_prepared, housing_label)

housing_predictions = forest_reg.predict(housing_prepared)

forest_mse = mean_squared_error(housing_label,housing_predictions)

forest_rmse = np.sqrt(forest_mse)

forest_rmse

#Parameter grid
param_grid = [
    {
        'n_estimators':[3,10,30],
        'max_features':[2,4,6,8]
    },
    
    {
        'bootstrap':[False],
        'n_estimators':[3,10],
        'max_features':[2,3,4]
    
    }
    
]

from sklearn.model_selection import GridSearchCV

grid_search = GridSearchCV(forest_reg,param_grid, cv =5, scoring='neg_mean_squared_error' )

grid_search.fit(housing_prepared, housing_label)

grid_search.best_params_

final_model = grid_search.best_estimator_
import joblib
joblib.dump(full_pipeline, "my_pipeline.pkl")
import joblib
joblib.dump(final_model, "my_model.pkl")
#Testing
X_test = test_set.drop("median_house_value", axis=1)
y_test = test_set["median_house_value"].copy()

X_test_prepared = full_pipeline.transform(X_test)

final_predictions = final_model.predict(X_test_prepared)

final_mse = mean_squared_error(y_test,final_predictions)

final_rmse = np.sqrt(final_mse)

final_rmse

